梯度：在机器学习中，我们通常定义一个损失函数，该损失函数衡量了模型预测值与真实值之间的差异。损失函数在某一点处的斜率，表示损失函数在该点处的变化率和方向。因此，梯度的方向指向了损失函数变化最快的方向，梯度的反方向指向了损失函数变化最慢的方向。

梯度下降：梯度下降是一种用于优化损失函数的迭代算法，我们需要找到使损失函数最小化的模型参数。从任意初始点开始，计算损失函数相对于这些参数的梯度，沿着梯度的反方向移动一定的步长，然后更新模型参数的值，使得损失函数不断减小，不断迭代此过程直到达到局部最小值或全局最小值。



# 优化算法

常见的优化方法有：梯度下降法（Gradient Descent）、随机梯度下降法（Stochastic Gradient Descent）、Adam方法

## 梯度下降法（Gradient Descent）

### 批量梯度下降（Batch Gradient Descent）

最重要的一种方法，也是很多其他优化算法的基础。
$$
\theta = \theta - \eta \nabla J(\theta)
$$
对于凸目标函数，肯定可以找到全局最优；对于非凸目标函数，可以找到局部最优

### 随机梯度下降法（Stochastic Gradient Descent）

$$
\theta = \theta - \eta \nabla J(\theta;x_{i},y_{i})
$$

梯度下降法是每次更新都计算整个数据集的loss，而随机梯度下降法每次更新都只用了一对样本，即上面公式中的一对样本$(x_{i},y_{i})$。由于每个样本都会对模型进行更新，所以模型更新的特别频繁，参数就会变成高方差，损失函数的波动也会有很大强度的变化。有时候，这是好事，因为这样的可以帮助我们探索新的更新方向，找到更加好的局部极值点。但是，由于频繁的更新和波动，会导致模型的损失收敛的非常不稳定。

(核心：随机选择一对样本来近似所有样本)

### 小批量梯度下降（Mini-batch Gradient Descent）

在每次更新时用b个样本，其实批量的梯度下降就是一种折中的方法
$$
\theta = \theta - \eta \frac{1}{B}\sum_{i=1}^{B} \nabla J(\theta;x_{i},y_{i})
$$
B就是自己设定的batch size的大小。

## 动量法（Momentum）

引入了momentum量，所以能够对梯度下降法起到加速的作用。
$$
V(t)=\gamma V(t-1)+\eta \nabla J(\theta) \\
\theta = \theta - V(t)
$$
当当前的梯度方向（$ΔJ(θ)$的正负号）和 $V ( t − 1 )$的方向相同时，$V ( t ) \gt η Δ J ( θ ) $所以参数  $\theta$的变化幅度会增大，从而加快梯度下降法的幅度；

而当方向不同时，会逐步减小当前更新的幅度。这样可以有效的对梯度下降法进行加速，同时提高模型的稳定性。

## 自适应梯度Adagrad（Adaptive Gradient）

Adagrad（Adaptive Gradient）是一种自适应学习率的优化算法，用于在神经网络训练过程中更新参数。

它的主要思想是为每个参数分配不同的学习率，根据参数的历史梯度信息来自动调整学习率的大小。Adagrad在梯度下降的基础上进行改进，可以有效地处理稀疏特征和非平稳问题。
$$
g_{i,t}=g_{i,t-1}  + \eta \nabla J(\theta_{i,t})^2\\
\theta_{i,t}=\theta_{i,t-1}  - \frac{\eta}{\sqrt{g_{i,t}+\epsilon }} · \nabla J(\theta_{i,t})
$$


其中$\nabla J(\theta_{i,t})$代表第i个参数在第t个迭代中的梯度，$\epsilon$是一个微小量，为了防止分母为0，一般取1e-8

下面是Adagrad算法的具体步骤：

1. 初始化参数：对每个参数w，初始化累积梯度平方和变量v为0。
2. 在每个训练迭代中，计算参数的梯度：根据损失函数对参数的梯度计算每个参数的当前梯度g。
3. 更新累积梯度平方和g：对于每个参数w，将当前梯度g的平方累加到累积梯度平方和变量中。
4. 计算参数更新：对于每个参数w，计算参数的更新量delta，使用学习率eta除以累积梯度平方和的平方根的值。然后将参数更新为w = w - delta。

重要的是，Adagrad的主要优势在于能够自动适应参数的学习率，并在训练过程中对于稀疏特征有较大的学习率，对于频繁出现的特征有较小的学习率。这使得Adagrad在处理不同尺度的梯度和非平稳问题时具有一定的鲁棒性。

然而，Adagrad也存在一些缺点。由于累积梯度平方和的累加，学习率在训练的早期会不断减小，可能导致学习率过小，使得模型在后续训练中难以继续收敛。为了解决这个问题，后续的优化算法如RMSProp和Adam进行了改进，引入了衰减系数和动量等机制来平衡历史梯度和当前梯度的影响。

## 自适应矩估计Adam(Adaptive Moment Estimation)

Adam（Adaptive Moment Estimation）是一种自适应学习率的优化算法，结合了动量法（Momentum）和RMSProp（Root Mean Square Propagation）的优点。Adam算法在梯度下降的基础上进行改进，可以有效地调整学习率，并在训练过程中自适应地调整参数更新的方向和步长。

1.动量项的指数加权平均
$$
m_{t}=β_{1}⋅m_{t−1}+(1−β_{1})⋅g_{t}
$$

这个公式计算了时间步 *t* 上梯度的指数移动平均。它使用了上一个移动平均值 $m_{t-1}$和当前的梯度$g_t$。参数 *β*1 控制移动平均的衰减率（算法作者建议设为0.9）。

2.累积梯度平方和的指数加权平均
$$
v_{t}=\beta_{2}·v_{t-1}+(1-\beta_2)·g_{t}^2
$$

这个公式计算了时间步t上梯度平方的指数移动平均。它也使用了上一个移动平均值$v_{t-1}$ 和当前的梯度平方${g_{t}^2}$。参数 *β*2 控制移动平均的衰减率（算法作者建议设为0.999）。

3.移动平均的偏差校正：
$$
\hat{m}_{t}=\frac{m_{t}}{1-\beta_{1}^t}\\
\hat{v}_{t}=\frac{v_{t}}{1-\beta_{2}^t}\\
$$
这两个公式对移动平均进行偏差校正，以纠正它们的初始偏差。将 $\beta_{1}^t$和$\beta_{2}^t$的项分别提高到当前时间步 *t* 的幂次来校正移动平均。

4.参数的更新：
$$
\theta_{t+1}=\theta-\frac {\alpha}{\sqrt{\hat{v_t}}+\epsilon} ·\hat{m_t}
$$
这个公式根据计算得到的移动平均值，将时间步*t* 上的参数 (*θ*) 更新为时间步 *t*+1 的参数。学习率 (*α*) 控制更新的步长。分母 $\sqrt{\hat{v_t}}+\epsilon$ 用于数值稳定性，并避免除以零。

下面是Adam算法的具体步骤：

1. 初始化参数：对每个参数w，初始化动量项m为0和累积梯度平方和v为0，以及时间步t为0。
2. 在每个训练迭代中，计算参数的梯度：根据损失函数对参数的梯度计算每个参数的当前梯度g。
3. 更新时间步：将时间步t增加1。
4. 计算动量项的指数加权平均：使用动量衰减率beta1对动量项m进行指数加权平均，更新m为m = beta1 * m + (1 - beta1) * g。
5. 计算累积梯度平方和的指数加权平均：使用梯度平方的指数加权平均系数beta2对累积梯度平方和v进行更新，更新v为v = beta2 * v + (1 - beta2) * g^2。
6. 修正偏差：由于m和v在初始阶段可能偏向较小的值，需要进行修正。计算修正后的动量项m_hat为m_hat = m / (1 - beta1^t)，计算修正后的累积梯度平方和v_hat为v_hat = v / (1 - beta2^t)。
7. 计算参数更新：根据修正后的动量项m_hat和修正后的累积梯度平方和v_hat计算参数的更新量delta，使用学习率alpha除以v_hat的平方根，再乘以动量项m_hat。然后将参数更新为w = w - delta。

重要的是，Adam算法通过动量项m和累积梯度平方和v的指数加权平均来自适应地调整学习率和参数的更新步长。它具有较好的鲁棒性，可以适应不同尺度的梯度，并在训练过程中自适应地调整学习率大小。此外，Adam算法还可以通过调整动量衰减率beta1和梯度平方的指数加权平均系数beta2来进一步调节算法的性能。

总体上，Adam算法结合了动量和梯度平方的信息，具有较好的收敛性和适应性，在深度学习中被广泛使用。









> 参考
>
> https://blog.csdn.net/autocyz/article/details/83114245
>
> https://zhuanlan.zhihu.com/p/357963858
>
> https://zhuanlan.zhihu.com/p/72039430
>
> http://zh.gluon.ai/chapter_optimization/optimization-intro.html